{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from collections import Counter \n",
    "import itertools\n",
    "import matplotlib as plt\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Cleaning\n",
    "## 2.1: Cleaning the data\n",
    "A collection of functions used for cleaning the data as according to the description of the milestone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads data\n",
    "def readData(path, size):\n",
    "    chunklist = []\n",
    "    i= 1\n",
    "    for chunk in pd.read_csv(path, sep=',', error_bad_lines=False, index_col=False, chunksize = size):\n",
    "        cleaner(chunk)\n",
    "        chunklist.append(chunk)\n",
    "        print(\"Chunk cleaned\", i)\n",
    "        i += 1\n",
    "    return pd.concat(chunklist)\n",
    "\n",
    "\n",
    "#Function to find and replace URLs with <URL>\n",
    "urlPattern = r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*'\n",
    "def swapUrl(line):\n",
    "    line = re.sub(urlPattern,' <URL> ', line)\n",
    "    return line\n",
    "\n",
    "#Function to find and replace dates with <DATE>\n",
    "re1 = re.compile(r'[\\d]{1,2}(th)? [adfjmnos]\\w*[,]?[.]? ([\\d]{2,4})?')\n",
    "re2 = re.compile(r'[adfjmnos]\\w*[,]?[.]? [\\d]{1,2}(th)?[,]? ([\\d]{2,4})?')\n",
    "re3 = re.compile(r'[adfjmnos]\\w* [\\d]{1,2}[,]?[.]?([\\d]{2,4})?')\n",
    "re4 = re.compile(r'[\\d]{1,2}-[\\d]{1,2}-[\\d]{2,4}')\n",
    "re5 = re.compile(r'[\\d]{1,2}/[\\d]{1,2}/[\\d]{2,4}')\n",
    "re6 = re.compile(r'[\\d]{1,2} [\\d]{1,2} [\\d]{2,4}')\n",
    "re7 = re.compile(r'[\\d].{1,2}.[\\d]{1,2}.[\\d]{2,4}')\n",
    "finReg = [re1, re2, re3, re4, re5, re6, re7]\n",
    "def swapDates(line):\n",
    "    for reg in finReg:\n",
    "        line = re.sub(reg, ' <DATE> ', line)\n",
    "    return line\n",
    "\n",
    "#Function to find and replace numbers with <NUM>\n",
    "pattern = r'[\\d]+[,]?([\\d]+)?'\n",
    "def swapNumb(line):\n",
    "    line = re.sub(pattern, ' <NUM> ', line)\n",
    "    return line\n",
    "\n",
    "#Main function for cleaning the data\n",
    "def cleaner(rawData):\n",
    "    #Removing rows without articleID\n",
    "    for index, row in rawData.iterrows():\n",
    "        if str(row['id']).isdigit():\n",
    "            continue\n",
    "        else:\n",
    "            rawData.drop(index, inplace=True)\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    #Tokenizes the content & change urls, date n numb.\n",
    "    for index, row in rawData.iterrows():\n",
    "        row['content'] = row['content'].lower()\n",
    "        row['content'] = re.sub(pattern, ' ', row['content'])\n",
    "        row['content'] = swapUrl(row['content'])\n",
    "        row['content'] = swapDates(row['content'])\n",
    "        row['content'] = swapNumb(row['content'])\n",
    "    metaList = []\n",
    "    #Reformats the row where meta_keywords are empty\n",
    "    for line in rawData['meta_keywords']:\n",
    "        if (line ==  \"['']\"):\n",
    "            metaList.append(np.nan)\n",
    "        else: \n",
    "            metaList.append(line)\n",
    "    rawData['meta_keywords'] = metaList\n",
    "    return rawData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Structuring the data and creating .csv files.\n",
    "Below is block of code which loads the set of 1 million news article and structures it according to our database schema and takes care of eventuall type errors and alike. There is alot of code, but most of it is pretty much the same only with different values and pairing different IDs up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk cleaned 1\n"
     ]
    }
   ],
   "source": [
    "df = readData('news_sample.csv', 20000)\n",
    "allTags = []\n",
    "allMeta = []\n",
    "allAuthors = []\n",
    "# Some further cleaning\n",
    "for i in range(len(df.index)):\n",
    "    line = str(df['tags'].iloc[i]).lower()\n",
    "    line2 = str(df['meta_keywords'].iloc[i]).lower()\n",
    "    line3 = str(df['authors'].iloc[i]).lower()\n",
    "    line, line2, line3 = line.replace('[', ''), line2.replace('[', ''), line3.replace('[', '')\n",
    "    line, line2, line3 = line.replace(']', ''), line2.replace(']', ''), line3.replace(']', '')\n",
    "    line, line2, line3 = line.split(', '), line2.split(', '), line3.split(', ')\n",
    "    allAuthors.append(line3)\n",
    "    allMeta.append(line2)\n",
    "    allTags.append(line)\n",
    "allAuthors = (list(itertools.chain.from_iterable(allAuthors)))\n",
    "allTags = (list(itertools.chain.from_iterable(allTags)))\n",
    "allMeta= (list(itertools.chain.from_iterable(allMeta)))\n",
    "authorList = list(dict.fromkeys(allAuthors))\n",
    "metaList = list(dict.fromkeys(allMeta))\n",
    "tagList = list(dict.fromkeys(allTags))\n",
    "tagDict = {}\n",
    "# Making dictionaries for tags, authors and meta keywords.\n",
    "for i in range (len(tagList)):\n",
    "    tagDict[tagList[i]] = i + 1\n",
    "tagDict.update({'nan':0})\n",
    "metaDict = {}\n",
    "for i in range (len(metaList)):\n",
    "    metaDict[metaList[i]] = i + 1\n",
    "metaDict.update({'nan':0})\n",
    "authorDict = {}\n",
    "for i in range (len(authorList)):\n",
    "    authorDict[authorList[i]] = i + 1\n",
    "authorDict.update({'nan':0})\n",
    "\n",
    "articleTagList = []\n",
    "metaKeyList = []\n",
    "authorIdList = []\n",
    "# Making dictionaries by using IDs to pair too articleID.\n",
    "# The dictionaries also gets written as csv files for the database.\n",
    "for i in range(len(df.index)):\n",
    "    article_tags = df['tags'].iloc[i]\n",
    "    meta_keys = df['meta_keywords'].iloc[i]\n",
    "    articleId = df['id'].iloc[i]\n",
    "    author_s = df['authors'].iloc[i]\n",
    "    # If there are no tags, tagID = 0\n",
    "    if isinstance(article_tags, float):\n",
    "        row = {'tagID': 0, 'articleID': articleId}\n",
    "        articleTagList.append(row)\n",
    "    else:\n",
    "        article_tags = article_tags.lower().split(', ')\n",
    "        for tag in article_tags:  \n",
    "            tag = tag.replace('[', '')\n",
    "            tag = tag.replace(']', '')      \n",
    "            tagId = int(tagDict[tag])\n",
    "            row = {'tagID': tagId, 'articleID': articleId}\n",
    "            articleTagList.append(row)\n",
    "    # If there are no meta keyword, meta_keyID = 0\n",
    "    if isinstance(meta_keys, float):\n",
    "        row = {'meta_keyID': 0, 'articleID': articleId}\n",
    "        metaKeyList.append(row) \n",
    "    else: \n",
    "        meta_keys = meta_keys.lower().split(', ') \n",
    "        for keyword in meta_keys:\n",
    "            keyword = keyword.replace('[', '')\n",
    "            keyword = keyword.replace(']', '')  \n",
    "            keyID = metaDict[keyword]\n",
    "            row = {'meta_keyID': keyID, 'articleID': articleId}\n",
    "            metaKeyList.append(row)\n",
    "    # If there are no authors, authorID = 0\n",
    "    if isinstance(author_s, float):\n",
    "        row = {'authorID': 0, 'articleID': articleId}\n",
    "        authorIdList.append(row) \n",
    "    else: \n",
    "        author_s = author_s.lower().split(', ') \n",
    "        for author in author_s:\n",
    "            author = author.replace('[', '')\n",
    "            author = author.replace(']', '')  \n",
    "            authorId = authorDict[author]\n",
    "            row = {'authorID': authorId, 'articleID': articleId}\n",
    "            authorIdList.append(row)\n",
    "# Loads the dictionaries to csv files.\n",
    "authorDict = {y:x for x,y in authorDict.items()}\n",
    "tagDict = {y:x for x,y in tagDict.items()}\n",
    "metaDict = {y:x for x,y in metaDict.items()}\n",
    "authorFrame = pd.DataFrame(list(authorDict.items()), columns = ['authorID', 'name'])\n",
    "tagFrame = pd.DataFrame(list(tagDict.items()), columns = ['tagID', 'tag'])\n",
    "keyFrame = pd.DataFrame(list(metaDict.items()), columns = ['meta_keyID', 'meta_keyword'])\n",
    "authorFrame.to_csv('author_name.csv', index=False)\n",
    "tagFrame.to_csv('tag_tag.csv', index=False)\n",
    "keyFrame.to_csv('key_id_word.csv', index=False)\n",
    "\n",
    "# Load csv files.\n",
    "article_tag = pd.DataFrame(articleTagList)\n",
    "article_tag.to_csv('article_tag.csv', index=False)\n",
    "\n",
    "article_metaKey = pd.DataFrame(metaKeyList)\n",
    "article_metaKey.to_csv('met_key_article.csv', index=False)\n",
    "\n",
    "authorIdFrame = pd.DataFrame(authorIdList)\n",
    "authorIdFrame.to_csv('authorID.csv', index=False)\n",
    "\n",
    "\n",
    "# Create dictionaries for loading to csv.\n",
    "df['type']=df['type'].fillna('NULL')\n",
    "typeDict = df.type.drop_duplicates().to_dict()\n",
    "domainDict = df.domain.drop_duplicates().to_dict()\n",
    "typeDict = {y:x for x,y in typeDict.items()}\n",
    "domainDict = {y:x for x,y in domainDict.items()}\n",
    "\n",
    "scrapeDict = df.scraped_at.drop_duplicates().to_dict()\n",
    "insertDict = df.inserted_at.drop_duplicates().to_dict()\n",
    "updatedDict = df.updated_at.drop_duplicates().to_dict()\n",
    "\n",
    "scrapeDict = {y:x for x,y in scrapeDict.items()}\n",
    "insertDict = {y:x for x,y in insertDict.items()}\n",
    "updatedDict = {y:x for x,y in updatedDict.items()}\n",
    "timeDict = {**scrapeDict, **insertDict, **updatedDict}\n",
    "i = 1 \n",
    "#Giving timestamps IDs\n",
    "for key  in timeDict:\n",
    "    timeDict[key] = i*3\n",
    "    i += 1\n",
    "\n",
    "# Creating csv files.\n",
    "df['domainID'] = df.apply(lambda row: domainDict[row['domain']], axis = 1)\n",
    "df['typeID'] = df.apply(lambda row: typeDict[row['type']], axis= 1)\n",
    "df['scrapedID'] = df.apply(lambda row: timeDict[row['scraped_at']], axis=1)\n",
    "df['insertedID'] = df.apply(lambda row: timeDict[row['inserted_at']], axis=1)\n",
    "df['updatedID'] = df.apply(lambda row: timeDict[row['updated_at']], axis= 1)\n",
    "\n",
    "Articles = df[['id', 'title','url','content','summary','scrapedID', 'insertedID', 'updatedID', 'meta_description']].copy()\n",
    "Articles.rename(columns={\"id\" : \"articleID\"}, inplace=True)\n",
    "Articles.to_csv('articles.csv', index=False)\n",
    "\n",
    "timeDict = {y:x for x,y in timeDict.items()}\n",
    "TimeStamps = pd.DataFrame(list(timeDict.items()), columns=['timeID', 'timestamp'], )\n",
    "TimeStamps.to_csv('timestamps.csv', index=False)\n",
    "\n",
    "typeDict = {y:x for x,y in typeDict.items()}\n",
    "types = pd.DataFrame(list(typeDict.items()), columns = ['typeID', 'type'])\n",
    "types.to_csv('Types.csv', index=False)\n",
    "\n",
    "DomainTypes = df[['domainID','domain', 'typeID']].copy()\n",
    "DomainTypes.drop_duplicates(subset='domainID', inplace=True)\n",
    "DomainTypes.to_csv('domain_types.csv', index=False)\n",
    "\n",
    "Domains = df[['id','domainID']].copy()\n",
    "Domains.rename(columns={\"id\":\"articleID\"}, inplace=True)\n",
    "Domains.to_csv('domains.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "## 5.1 Spider\n",
    "In order to scrape wikipedia for articels we have used the scrapy framework. Below is the code for our scrapy.Spider which scrapes the article obtaining the HTML code. Then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
